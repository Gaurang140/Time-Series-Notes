{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset for Training Using TensorFlow\n",
    "\n",
    "In this guide, we will prepare a dataset for training using TensorFlow. We will perform several key steps to get the data ready for training a machine learning model.\n",
    "\n",
    "## 1. Create the Dataset\n",
    "\n",
    "First, we will create a dataset with a range of 10 elements using TensorFlow's `tf.data.Dataset.range(10)`.\n",
    "\n",
    "## 2. Windowing and Shifting\n",
    "\n",
    "Next, we will apply the windowing technique to our dataset. We can define the size of each window and the shift between consecutive windows. We'll explore the impact of setting `drop_remainder` to `True`.\n",
    "\n",
    "- **Window Size**: We will specify the size of each window, which determines how many elements are grouped together in each window.\n",
    "\n",
    "- **Shift**: The shift parameter defines how the window moves forward after creating each window. A shift of 1 means the windows will overlap by one element.\n",
    "\n",
    "- **drop_remainder=True**: Setting this parameter to `True` will drop any incomplete windows at the end of the dataset if there are fewer elements than the specified window size.\n",
    "\n",
    "## 3. Flattening the Dataset\n",
    "\n",
    "To further process the data, we will flatten the dataset of windows. We'll use the `flat_map` function to apply a lambda function to each window and then concatenate the results into a single dataset. This is typically done to convert windows of data into individual elements for easier handling.\n",
    "\n",
    "## 4. Feature Engineering and Label Creation\n",
    "\n",
    "After flattening the dataset, we can use the `map` function to apply transformations to the data. This is often used for feature engineering and label creation. You can define custom functions to modify or extract features from the data.\n",
    "\n",
    "## 5. Data Shuffling\n",
    "\n",
    "Shuffling the data is a good practice to reduce sequence bias when training a model. We'll shuffle the dataset to ensure that the order of examples doesn't affect the training process.\n",
    "\n",
    "## 6. Batching\n",
    "\n",
    "To train a model, we'll create batches of data. Batching groups several examples together into a single batch, which is more efficient for model training.\n",
    "\n",
    "## 7. Prefetching\n",
    "\n",
    "Finally, we'll use the `prefetch` function to prefetch data for the next batch. Prefetching helps in reducing training time by overlapping the data loading and model training phases.\n",
    "\n",
    "By following these steps, we'll have a well-prepared dataset ready for training machine learning models in TensorFlow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the data set with 10 elements \n",
    "dataset = tf.data.Dataset.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RangeDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for val in dataset:\n",
    "    print(val.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.window(size=5 , shift=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[1, 2, 3, 4, 5]\n",
      "[2, 3, 4, 5, 6]\n",
      "[3, 4, 5, 6, 7]\n",
      "[4, 5, 6, 7, 8]\n",
      "[5, 6, 7, 8, 9]\n",
      "[6, 7, 8, 9]\n",
      "[7, 8, 9]\n",
      "[8, 9]\n",
      "[9]\n"
     ]
    }
   ],
   "source": [
    "for window_data in dataset : \n",
    "    print([item.numpy() for item in window_data])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[1, 2, 3, 4, 5]\n",
      "[2, 3, 4, 5, 6]\n",
      "[3, 4, 5, 6, 7]\n",
      "[4, 5, 6, 7, 8]\n",
      "[5, 6, 7, 8, 9]\n",
      "[6, 7, 8, 9]\n",
      "[7, 8, 9]\n",
      "[8, 9]\n",
      "[9]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "for window_data in dataset:\n",
    "    \n",
    "    list2 = []\n",
    "    for item in window_data:\n",
    "        list2.append(item.numpy())\n",
    "    print(list2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make dataset only return 5 elements we could use drop remainder\n",
    "dataset = tf.data.Dataset.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.window(size=5 , shift=1 , drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=int64, numpy=0>, <tf.Tensor: shape=(), dtype=int64, numpy=1>, <tf.Tensor: shape=(), dtype=int64, numpy=2>, <tf.Tensor: shape=(), dtype=int64, numpy=3>, <tf.Tensor: shape=(), dtype=int64, numpy=4>]\n",
      "[<tf.Tensor: shape=(), dtype=int64, numpy=1>, <tf.Tensor: shape=(), dtype=int64, numpy=2>, <tf.Tensor: shape=(), dtype=int64, numpy=3>, <tf.Tensor: shape=(), dtype=int64, numpy=4>, <tf.Tensor: shape=(), dtype=int64, numpy=5>]\n",
      "[<tf.Tensor: shape=(), dtype=int64, numpy=2>, <tf.Tensor: shape=(), dtype=int64, numpy=3>, <tf.Tensor: shape=(), dtype=int64, numpy=4>, <tf.Tensor: shape=(), dtype=int64, numpy=5>, <tf.Tensor: shape=(), dtype=int64, numpy=6>]\n",
      "[<tf.Tensor: shape=(), dtype=int64, numpy=3>, <tf.Tensor: shape=(), dtype=int64, numpy=4>, <tf.Tensor: shape=(), dtype=int64, numpy=5>, <tf.Tensor: shape=(), dtype=int64, numpy=6>, <tf.Tensor: shape=(), dtype=int64, numpy=7>]\n",
      "[<tf.Tensor: shape=(), dtype=int64, numpy=4>, <tf.Tensor: shape=(), dtype=int64, numpy=5>, <tf.Tensor: shape=(), dtype=int64, numpy=6>, <tf.Tensor: shape=(), dtype=int64, numpy=7>, <tf.Tensor: shape=(), dtype=int64, numpy=8>]\n",
      "[<tf.Tensor: shape=(), dtype=int64, numpy=5>, <tf.Tensor: shape=(), dtype=int64, numpy=6>, <tf.Tensor: shape=(), dtype=int64, numpy=7>, <tf.Tensor: shape=(), dtype=int64, numpy=8>, <tf.Tensor: shape=(), dtype=int64, numpy=9>]\n"
     ]
    }
   ],
   "source": [
    "for window_data in dataset:\n",
    "    print([item.numpy()) for item in window_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use flattern the dataset\n",
    "\n",
    "dataset = dataset.flat_map(lambda window : window.batch(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "[1 2 3 4 5]\n",
      "[2 3 4 5 6]\n",
      "[3 4 5 6 7]\n",
      "[4 5 6 7 8]\n",
      "[5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "for window_data in dataset:\n",
    "    print(window_data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[2 3 4 5]\n",
      " [3 4 5 6]]\n",
      "y =  [6 7]\n",
      "\n",
      "x =  [[1 2 3 4]\n",
      " [5 6 7 8]]\n",
      "y =  [5 9]\n",
      "\n",
      "x =  [[4 5 6 7]\n",
      " [0 1 2 3]]\n",
      "y =  [8 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group into feature and lable \n",
    "\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.window( 5 , shift=1 , drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window : window.batch(5))\n",
    "dataset = dataset.map(lambda mywindow : (mywindow[:-1] ,mywindow[-1]))\n",
    "\n",
    "\n",
    "# suffle the data it is good practice to shuffle your to reduce sequence bias \n",
    "dataset = dataset.shuffle(buffer_size=10)\n",
    "\n",
    "\n",
    "\n",
    "# Create batches of windows\n",
    "dataset = dataset.batch(2).prefetch(1)   # by specifying a prefetch buffer size of 1 tensorflow will prepare the next batch in advance \n",
    "\n",
    "# Print the results\n",
    "for x,y in dataset:\n",
    "  print(\"x = \", x.numpy())\n",
    "  print(\"y = \", y.numpy())\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
